# -*- coding: utf-8 -*-
"""Predictive Analysis SpaceX.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1p7jkrdP-C4dk1aPz0z_Ev-0NwGvfJxV1

Objectives
Perform exploratory Data Analysis and determine Training Labels

create a column for the class
Standardize the data
Split into training data and test data
-Find best Hyperparameter for SVM, Classification Trees and Logistic Regression

Find the method performs best using test data
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import confusion_matrix

"""This function is to plot the confusion matrix."""

def plot_confusion_matrix(y,y_predict):
  cm = confusion_matrix(y,y_predict)
  ax = plt.subplot()
  sns.heatmap(cm,annot = True, ax = ax)
  plt.xlabel('Predicted Label')
  plt.ylabel('True Label')
  ax.set_xticklabels(['did not land', 'landed'])
  ax.set_yticklabels(['did not land', 'landed'])
  plt.show()

URL1 = "https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DS0321EN-SkillsNetwork/datasets/dataset_part_2.csv"
data = pd.read_csv(URL1)

data.head(10)

URL2 = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DS0321EN-SkillsNetwork/datasets/dataset_part_3.csv'
X = pd.read_csv(URL2)

X.head(100)

"""**TASK 1**

Create a NumPy array from the column Class in data, by applying the method to_numpy() then assign it to the variable Y,make sure the output is a Pandas series (only one bracket df['name of column']).


"""

Y = data['Class'].to_numpy()

"""**TASK 2**

Standardize the data in X then reassign it to the variable X using the transform provided below.
"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X = scaler.fit_transform(X)

"""**TASK 3**

Use the function train_test_split to split the data X and Y into training and test data. Set the parameter test_size to 0.2 and random_state to 2. The training data and test data should be assigned to the following labels.

X_train, X_test, Y_train, Y_test
"""

X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size = 0.2, random_state = 2)

Y_test.shape

"""**TASK 4**

Create a logistic regression object then create a GridSearchCV object logreg_cv with cv = 10. Fit the object to find the best parameters from the dictionary parameters
"""

parameters ={'C':[0.01,0.1,1],
             'penalty':['l2'],
             'solver':['lbfgs']}

lr = LogisticRegression()

logreg_cv = GridSearchCV(lr,parameters,cv = 10)

logreg_cv.fit(X_train,Y_train)

print("tuned hpyerparameters :(best parameters) ",logreg_cv.best_params_)
print("accuracy :",logreg_cv.best_score_)

"""TASK 5
Calculate the accuracy on the test data using the method score
"""

accuracy_score = logreg_cv.score(X_test,Y_test)
print(accuracy_score)

Yhat = logreg_cv.predict(X_test)

plot_confusion_matrix(Y_test,Yhat)

"""**TASK 6**

Create a support vector machine object then create a GridSearchCV object svm_cv with cv = 10. Fit the object to find the best parameters from the dictionary parameters.
"""

svm = SVC()
parameters = {'kernel':('linear', 'rbf','poly','rbf', 'sigmoid'),
              'C': np.logspace(-3, 3, 5),
              'gamma':np.logspace(-3, 3, 5)}

svm_cv = GridSearchCV(svm,parameters, cv = 10)

svm_cv.fit(X_train,Y_train)

print("tuned hpyerparameters :(best parameters) ",svm_cv.best_params_)
print("accuracy :",svm_cv.best_score_)

"""**TASK 7**

Calculate the accuracy on the test data using the method score
"""

test_accuracy1 = svm_cv.score(X_test,Y_test)
print(test_accuracy1)

Yhat = svm_cv.predict(X_test)

plot_confusion_matrix(Y_test,Yhat)

"""**TASK 8**

Create a decision tree classifier object then create a GridSearchCV object tree_cv with cv = 10. Fit the object to find the best parameters from the dictionary parameters
"""

tree = DecisionTreeClassifier()
parameters = {'criterion': ['gini', 'entropy'],
     'splitter': ['best', 'random'],
     'max_depth': [2*n for n in range(1,10)],
     'max_features': ['auto', 'sqrt'],
     'min_samples_leaf': [1, 2, 4],
     'min_samples_split': [2, 5, 10]}

tree_cv = GridSearchCV(tree,parameters,cv = 10)

tree_cv.fit(X_train,Y_train)

print("tuned hpyerparameters :(best parameters) ",tree_cv.best_params_)
print("accuracy :",tree_cv.best_score_)

"""**TASK 9**

Calculate the accuracy of tree_cv on the test data using the method score:
"""

print(tree_cv.score(X_test,Y_test))

Yhat = tree_cv.predict(X_test)
plot_confusion_matrix(Y_test,Yhat)

"""**TASK 10**

Create a k nearest neighbors object then create a GridSearchCV object knn_cv with cv = 10. Fit the object to find the best parameters from the dictionary parameters.
"""

KNN = KNeighborsClassifier()
parameters = {'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
              'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],
              'p': [1,2]}

knn_cv = GridSearchCV(KNN,parameters,cv = 10)

knn_cv.fit(X_train,Y_train)

print("tuned hpyerparameters :(best parameters) ",knn_cv.best_params_)
print("accuracy :",knn_cv.best_score_)

"""**TASK 11**

Calculate the accuracy of knn_cv on the test data using the method score:
"""

print(knn_cv.score(X_test,Y_test))

Yhat = knn_cv.predict(X_test)
plot_confusion_matrix(Y_test,Yhat)

scores = {
    'Logistic Regression' : logreg_cv.best_score_,
    'SVM': svm_cv.best_score_,
    'DecisionTree' : tree_cv.best_score_,
    'KNN' : knn_cv.best_score_
}

plt.figure(figsize = (8,6))
plt.bar(scores.keys(),scores.values(),color=['skyblue','lightgreen','salmon','orange'])
plt.ylabel('Best Score')
plt.title('Classification Accuracy')
plt.ylim(0,1)
plt.show()